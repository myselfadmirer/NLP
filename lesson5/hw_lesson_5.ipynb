{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPRx8Cu_RDY1",
    "outputId": "9595e8e4-9731-4cfe-a67f-cfb0d9759cd4"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vXxwW9NzW570"
   },
   "outputs": [],
   "source": [
    "# !mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpwgA3svWiRw",
    "outputId": "9d4e2aa6-bcc7-4793-f85d-2ce708f88ff8"
   },
   "outputs": [],
   "source": [
    "# !wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "# !wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for sent in full_train:\n",
    "    set_list = []\n",
    "    for token in sent:\n",
    "        set_list.append((token.form, token.upos))\n",
    "    train_data.append(set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "for sent in full_test:\n",
    "    set_list = []\n",
    "    for token in sent:\n",
    "        set_list.append((token.form, token.upos))\n",
    "    test_data.append(set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [],
   "source": [
    "test_sent = []\n",
    "\n",
    "for sent in full_test:\n",
    "    for token in sent:\n",
    "        test_sent.append(token.form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = []\n",
    "eval_ = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [],
   "source": [
    "unigram_tagger = UnigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = unigram_tagger.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('unigram_tagger')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', 'NOUN'),\n",
       " ('аль', 'PART'),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(unigram_tagger.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6963064064974893"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bigram_tagger.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('bigram_tagger')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', None),\n",
       " ('аль', None),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bigram_tagger.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BigramTagger в комбинации с UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger_2 = BigramTagger(train_data, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bigram_tagger_2.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('bigram_tagger_2')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', 'NOUN'),\n",
       " ('аль', 'PART'),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bigram_tagger_2.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger = TrigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24808748694099012"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = trigram_tagger.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('trigram_tagger')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', None),\n",
       " ('от', None),\n",
       " ('имени', None),\n",
       " ('учёного', None),\n",
       " ('аль', None),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', None),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trigram_tagger.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TrigramTagger в комбинации с UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger_2 = TrigramTagger(train_data, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881769622215482"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = trigram_tagger_2.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('trigram_tagger_2')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', 'NOUN'),\n",
       " ('аль', 'PART'),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trigram_tagger_2.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TrigramTagger в комбинации с BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger_3 = TrigramTagger(train_data, backoff=bigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6892039901594041"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = trigram_tagger_3.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('trigram_tagger_3')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', None),\n",
       " ('аль', None),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trigram_tagger_3.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TrigramTagger в комбинации с BigramTagger в комбинации с UnigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger_4 = TrigramTagger(train_data, backoff=bigram_tagger_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = trigram_tagger_4.evaluate(test_data)\n",
    "eval_.append(e)\n",
    "tagger.append('trigram_tagger_4')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Алгоритм', None),\n",
       " (',', 'PUNCT'),\n",
       " ('от', 'ADP'),\n",
       " ('имени', 'NOUN'),\n",
       " ('учёного', 'NOUN'),\n",
       " ('аль', 'PART'),\n",
       " ('-', 'PUNCT'),\n",
       " ('Хорезми', None),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trigram_tagger_4.tag(test_sent)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Лучший результат у BigramTagger в комбинации с UnigramTagger (0.8829828463586425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Castom tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "\n",
    "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer с параметром analyzer='char'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 149809)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459356991204125"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('tfidf_vectorizer')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer с параметром analyzer='word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_2 = TfidfVectorizer(ngram_range=(1, 1), analyzer='word', max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer_2.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer_2.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 98088)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7503370067064334"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('tfidf_vectorizer_2')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CountVectorizer с параметром analyzer='char'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = c_vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = c_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 149809)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532908704883227"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('c_vectorizer')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CountVectorizer с параметром analyzer='word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer_2 = CountVectorizer(ngram_range=(1, 1), analyzer='word', max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = c_vectorizer_2.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = c_vectorizer_2.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 98088)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.750311731203451"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('c_vectorizer_2')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer с параметром analyzer='char'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6781754456913692"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('hvectorizer')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer с параметром analyzer='word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer_2 = HashingVectorizer(ngram_range=(1, 1), analyzer='word', n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer_2.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer_2.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28609341825902335"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = accuracy_score(test_enc_labels, pred)\n",
    "eval_.append(e)\n",
    "tagger.append('hvectorizer_2')\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagger</th>\n",
       "      <th>eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trigram_tagger</td>\n",
       "      <td>0.248087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hvectorizer_2</td>\n",
       "      <td>0.286093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hvectorizer</td>\n",
       "      <td>0.678175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trigram_tagger_3</td>\n",
       "      <td>0.689204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigram_tagger</td>\n",
       "      <td>0.696306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c_vectorizer_2</td>\n",
       "      <td>0.750312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf_vectorizer_2</td>\n",
       "      <td>0.750337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram_tagger</td>\n",
       "      <td>0.877254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trigram_tagger_2</td>\n",
       "      <td>0.881770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trigram_tagger_4</td>\n",
       "      <td>0.882081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigram_tagger_2</td>\n",
       "      <td>0.882983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_vectorizer</td>\n",
       "      <td>0.945936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c_vectorizer</td>\n",
       "      <td>0.953291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tagger      eval\n",
       "3       trigram_tagger  0.248087\n",
       "12       hvectorizer_2  0.286093\n",
       "11         hvectorizer  0.678175\n",
       "5     trigram_tagger_3  0.689204\n",
       "1        bigram_tagger  0.696306\n",
       "10      c_vectorizer_2  0.750312\n",
       "8   tfidf_vectorizer_2  0.750337\n",
       "0       unigram_tagger  0.877254\n",
       "4     trigram_tagger_2  0.881770\n",
       "6     trigram_tagger_4  0.882081\n",
       "2      bigram_tagger_2  0.882983\n",
       "7     tfidf_vectorizer  0.945936\n",
       "9         c_vectorizer  0.953291"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({'tagger': tagger, 'eval': eval_})\n",
    "result.sort_values('eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer с параметром analyzer='char' показал лучший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cINqgGpKXURp"
   },
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM0drjKXYet"
   },
   "source": [
    "много дополнительных датасетов на русском языке\n",
    "\n",
    "https://natasha.github.io/corus/  \n",
    "https://github.com/natasha/corus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOg4C8sZNpw"
   },
   "source": [
    "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzi6ApNLZg6X"
   },
   "source": [
    "**Проверить насколько хорошо работает NER**\n",
    "\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы:\n",
    "* передаём в сетку токен и его соседей\n",
    "* передаём в сетку только токен\n",
    "\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP1LgaNUtaOz"
   },
   "source": [
    "при обучении своего нера не забудьте разделить выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qg6tcss2Zhp9",
    "outputId": "d9b8f633-32e1-4392-be8e-5aefe5ca0444"
   },
   "outputs": [],
   "source": [
    "# !pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "hrc5ocDkaS1e"
   },
   "outputs": [],
   "source": [
    "import corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPVv6lhT3ftA",
    "outputId": "ae2f6c07-afc5-4093-b843-eed25d6d749b"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtLK4z9CH2Ph",
    "outputId": "3abe264a-0f92-48d7-9eb5-76eafdafde8d"
   },
   "outputs": [],
   "source": [
    "# !unzip collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JPxCa2LyH8gW"
   },
   "outputs": [],
   "source": [
    "bigram_tagger = BigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7burz-bD3t4l",
    "outputId": "98119edc-289c-43e7-b67e-17d9ef901c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collection5\t   hw_lesson_5.ipynb    lesson_5.ipynb\n",
      " collection5.zip   image\t       'ДЗ 5 инструкция.docx'\n",
      " datasets\t   lesson5_actual.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEdS2pAS3fod",
    "outputId": "402714b1-6931-41ce-ef39-f68080d9a29e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='last_28',\n",
       "    text='Распределены кураторы госпрограмм в правительстве РФ\\r\\n\\r\\n\\r\\nБольше других \"работы\" досталось Игорю Шувалову, Ольге Голодец, Аркадию Дворковичу и Дмитрию Рогозину.\\r\\n\\r\\n\\r\\n Премьер Дмитрий Медведев распределил между своими заместителями функции кураторов реализации госпрограмм, сообщается на сайте правительства России.\\r\\n\\r\\nБольше других \"работы\" досталось Игорю Шувалову, Ольге Голодец, Аркадию Дворковичу и Дмитрию Рогозину. Шувалову и Голодец назначено по восемь программ, а Дворкович и Рогозин будут курировать по 11 проектов.\\r\\n\\r\\nКак уточняется на сайте кабмина, в частности вице-премьер по оборонной промышленности Рогозин будет отвечать за противодействие незаконному обороту наркотиков, развитие судостроения и космическую деятельность. Первый вице-премьер Шувалов стал ответственным за инновационную экономику, обеспечение россиян доступным жильем и управление государственными финансами.\\r\\n\\r\\nВице-премьер по соцполитике Ольга Голодец будет курировать, в частности, развитие науки и технологий, пенсионной системы и программу социальной поддержки граждан, а вице-премьер по реальной экономике Дворкович — развитие фармацевтики, лесного хозяйства и энергетики.\\r\\n\\r\\nДмитрий Козак будет отвечать за развитие физкультуры и спорта и региональную политику, глава аппарата правительства Сергей Приходько стал куратором программ по юстиции и развитию сферы массовых коммуникаций и СМИ, а президентский полпред в СКФО Александр Хлопонин будет решать вопросы развития Северного Кавказа.',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='GEOPOLIT',\n",
       "         start=50,\n",
       "         stop=52,\n",
       "         text='РФ'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='PER',\n",
       "         start=91,\n",
       "         stop=105,\n",
       "         text='Игорю Шувалову'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='PER',\n",
       "         start=107,\n",
       "         stop=120,\n",
       "         text='Ольге Голодец'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='PER',\n",
       "         start=122,\n",
       "         stop=140,\n",
       "         text='Аркадию Дворковичу'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='PER',\n",
       "         start=143,\n",
       "         stop=159,\n",
       "         text='Дмитрию Рогозину'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='PER',\n",
       "         start=175,\n",
       "         stop=191,\n",
       "         text='Дмитрий Медведев'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='GEOPOLIT',\n",
       "         start=307,\n",
       "         stop=313,\n",
       "         text='России'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='PER',\n",
       "         start=351,\n",
       "         stop=365,\n",
       "         text='Игорю Шувалову'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='PER',\n",
       "         start=367,\n",
       "         stop=380,\n",
       "         text='Ольге Голодец'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='PER',\n",
       "         start=382,\n",
       "         stop=400,\n",
       "         text='Аркадию Дворковичу'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='PER',\n",
       "         start=403,\n",
       "         stop=419,\n",
       "         text='Дмитрию Рогозину'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='PER',\n",
       "         start=421,\n",
       "         stop=429,\n",
       "         text='Шувалову'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='PER',\n",
       "         start=432,\n",
       "         stop=439,\n",
       "         text='Голодец'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='PER',\n",
       "         start=472,\n",
       "         stop=481,\n",
       "         text='Дворкович'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='PER',\n",
       "         start=484,\n",
       "         stop=491,\n",
       "         text='Рогозин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='PER',\n",
       "         start=614,\n",
       "         stop=621,\n",
       "         text='Рогозин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='PER',\n",
       "         start=758,\n",
       "         stop=765,\n",
       "         text='Шувалов'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T18',\n",
       "         type='PER',\n",
       "         start=922,\n",
       "         stop=935,\n",
       "         text='Ольга Голодец'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T19',\n",
       "         type='PER',\n",
       "         start=1094,\n",
       "         stop=1103,\n",
       "         text='Дворкович'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T20',\n",
       "         type='PER',\n",
       "         start=1164,\n",
       "         stop=1177,\n",
       "         text='Дмитрий Козак'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T21',\n",
       "         type='PER',\n",
       "         start=1280,\n",
       "         stop=1296,\n",
       "         text='Сергей Приходько'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T22',\n",
       "         type='MEDIA',\n",
       "         start=1373,\n",
       "         stop=1376,\n",
       "         text='СМИ'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T23',\n",
       "         type='LOC',\n",
       "         start=1404,\n",
       "         stop=1408,\n",
       "         text='СКФО'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T24',\n",
       "         type='PER',\n",
       "         start=1409,\n",
       "         stop=1427,\n",
       "         text='Александр Хлопонин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T25',\n",
       "         type='LOC',\n",
       "         start=1458,\n",
       "         stop=1475,\n",
       "         text='Северного Кавказа'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "\n",
    "dir = 'Collection5/'\n",
    "records = load_ne5(dir)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrhLNgNwQP2P"
   },
   "source": [
    "процедуры обработки взять из вебинарного ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "XDdnL6EXJRt9"
   },
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "skYaNCiC5xM4"
   },
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219046\n",
       "PER          21167\n",
       "ORG          13651\n",
       "LOC           4565\n",
       "GEOPOLIT      4354\n",
       "MEDIA         2481\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Опрос</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>РБК-Украина</td>\n",
       "      <td>MEDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word    tag\n",
       "0        Опрос    OUT\n",
       "1  РБК-Украина  MEDIA\n",
       "2            :    OUT"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deeppavlov посмотрела, но пришлось его удалить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kd-emBao1u-",
    "outputId": "a31344d7-177b-4b5f-b49e-4d4201f529ff"
   },
   "outputs": [],
   "source": [
    "# установка deeppavlov\n",
    "\n",
    "# !pip uninstall -y tensorflow tensorflow-gpu\n",
    "# !pip install numpy scipy librosa unidecode inflect librosa transformers\n",
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "KY96lqBzsZJ_"
   },
   "outputs": [],
   "source": [
    "#!python -m deeppavlov install squad_bert\n",
    "#!python -m deeppavlov install ner_ontonotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "6KE7tpVWs1b7"
   },
   "outputs": [],
   "source": [
    "# import deeppavlov\n",
    "# from deeppavlov import configs, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsegbgCbrzy_",
    "outputId": "98fbfd4e-0fff-433a-9c24-2fa56ab7d241"
   },
   "outputs": [],
   "source": [
    "# deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
    "# rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
    "# deeppavlov_ner([rus_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "XQ1phPKisJMz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265264, 2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.apply(len).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3207      возглавил\n",
       "200339            -\n",
       "220898           На\n",
       "10439     отношении\n",
       "3292      Австралии\n",
       "            ...    \n",
       "98614       состава\n",
       "242153        более\n",
       "176571            ,\n",
       "251917            .\n",
       "63191            их\n",
       "Name: word, Length: 66316, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 06:11:37.273589: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-23 06:11:37.273628: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-23 06:11:37.273650: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mitko-pc): /proc/driver/nvidia/version does not exist\n",
      "2021-09-23 06:11:37.274400: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 06:12:30.000394: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    #ngrams=(1, 3),\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29834"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12435/12435 [==============================] - 246s 20ms/step - loss: 0.2934 - accuracy: 0.9145 - val_loss: 0.2157 - val_accuracy: 0.9371\n",
      "Epoch 2/3\n",
      "12435/12435 [==============================] - 235s 19ms/step - loss: 0.1250 - accuracy: 0.9631 - val_loss: 0.2195 - val_accuracy: 0.9394\n",
      "Epoch 3/3\n",
      "12435/12435 [==============================] - 234s 19ms/step - loss: 0.1089 - accuracy: 0.9659 - val_loss: 0.2349 - val_accuracy: 0.9394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11bf38ceb0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel = modelNER()\n",
    "\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accurasy = 0,9394 - это меньше, чем у CountVectorizer с параметром analyzer='char'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
